agent:
  service_name: telegen

selfTelemetry:
  listen: ":19090"
  prometheus_namespace: "telegen"

cloud:
  aws:
    enabled: true
    timeout: "200ms"
    refresh_interval: "15m"
    collect_tags: false
    tag_allowlist: []

queues:
  traces:  { mem_limit: "256Mi", max_age: "6h" }
  logs:    { mem_limit: "256Mi", max_age: "24h" }
  metrics: { mem_limit: "128Mi", max_age: "5m" }

backoff:
  initial: "500ms"
  max: "30s"
  multiplier: 2.0
  jitter: 0.2

# =============================================================================
# Export Configuration - OTel Collector Compatible
# =============================================================================
# All signals include telegen.* metadata attributes for indexing:
#   - telegen.signal.category
#   - telegen.signal.subcategory
#   - telegen.source.module
#   - telegen.bpf.component
#   - telegen.signal.description
#   - telegen.collector.type
# =============================================================================

exports:
  # Include signal metadata in all exports for indexing
  include_signal_metadata: true
  
  # Control which metadata fields are exported (to reduce storage costs)
  # Each field can be independently enabled/disabled
  # Can also be set via environment variables:
  #   TELEGEN_METADATA_CATEGORY, TELEGEN_METADATA_SUBCATEGORY, etc.
  metadata_fields:
    # Top-level category (e.g., "Host Metrics", "Database Traces")
    enable_category: true
    # Sub-category (e.g., "CPU Utilization", "PostgreSQL")
    enable_subcategory: true
    # Go source module path
    enable_source_module: true
    # eBPF component file path
    enable_bpf_component: true
    # Human-readable description (verbose, disabled by default)
    enable_description: false
    # Collector type (ebpf, jfr, snmp, api, procfs, nvml)
    enable_collector_type: true
  
  # Prometheus Remote Write - compatible with OTel Collector prometheusremotewritereceiver
  remoteWrite:
    mode: "active"
    # OTel Collector prometheusremotewritereceiver endpoint
    endpoints:
      - url: "http://otel-collector:19291/api/v1/push"
        timeout: "30s"
        headers: {}
        tenant: ""
        # snappy is standard for Prometheus Remote Write
        compression: "snappy"
    tls:
      enable: false
      ca_file: ""
      cert_file: ""
      key_file: ""
      insecure_skip_verify: false
    batch:
      size: 1000
      flush_interval: "15s"
    retry:
      max_retries: 3
      backoff: "1s"

  # OTLP export - compatible with OTel Collector otlpreceiver
  otlp:
    send_mode: "failover"
    tls:
      enable: false
      ca_file: ""
      cert_file: ""
      key_file: ""
      insecure_skip_verify: false
    # gRPC endpoint for traces, metrics, and logs
    grpc:
      enabled: true
      endpoint: "otel-collector:4317"
      headers: {}
      insecure: true
      compression: "gzip"
      timeout: "30s"
    # HTTP endpoint (alternative to gRPC)
    http:
      enabled: false
      endpoint: "otel-collector:4318"
      insecure: true
      traces_path: "/v1/traces"
      logs_path: "/v1/logs"
      metrics_path: "/v1/metrics"
      headers: {}
      compression: "gzip"
      timeout: "30s"

pipelines:
  metrics:
    also_expose_prometheus: true
  traces:
    enabled: true
  logs:
    enabled: true
    filelog:
      include: ["/var/log/*.log", "/var/log/*/*.log"]
      position_file: "/var/lib/telegen/positions.json"
  
  # =============================================================================
  # Kafka Logs Receiver - Stream logs from Kafka topics
  # =============================================================================
  # Consumes raw log messages from Kafka, parses them using telegen's
  # multi-format parser (Docker JSON, CRI-O, Spring Boot, Log4j, etc.),
  # and exports via the shared OTLP exporter.
  # =============================================================================
  kafka:
    enabled: false
    
    # Kafka broker addresses
    brokers:
      - "localhost:9092"
    
    # Consumer group ID for coordinated consumption
    group_id: "telegen-logs"
    
    # Client ID (optional, auto-generated if empty)
    client_id: ""
    
    # Topics to consume (supports wildcards)
    topics:
      - "application-logs"
      - "system-logs"
    
    # Topics to exclude (regex patterns)
    exclude_topics:
      - ".*-debug$"
      - "test-.*"
    
    # Starting position: "earliest" or "latest"
    initial_offset: "latest"
    
    # Consumer group timeouts
    session_timeout: "10s"
    heartbeat_interval: "3s"
    rebalance_timeout: "30s"
    
    # Partition assignment strategy:
    # "range", "roundrobin", "sticky", "cooperative-sticky"
    group_rebalance_strategy: "cooperative-sticky"
    
    # Offset commit behavior
    message_marking:
      after: true                # Commit after successful processing
      on_error: false            # Commit even on transient errors
      on_permanent_error: true   # Commit on permanent errors (e.g., parse failures)
    
    # Batch settings
    batch:
      size: 100                  # Max messages per batch
      timeout: "500ms"           # Max batch wait time
      max_partition_bytes: 1048576  # 1MB max per partition
    
    # Log parsing configuration
    parser:
      enable_runtime_parsing: true       # Docker JSON, CRI-O, containerd
      enable_application_parsing: true   # Spring Boot, Log4j, JSON
      enable_k8s_enrichment: true        # Kubernetes metadata extraction
      enable_trace_context_enrichment: true  # eBPF trace correlation
      default_severity: "INFO"
    
    # Telemetry metrics (exposed via /metrics endpoint)
    telemetry:
      kafka_receiver_records: true       # Records received/processed/failed
      kafka_receiver_offset_lag: true    # Consumer lag per partition
      kafka_receiver_records_delay: true # Message age histogram
      kafka_broker_connects: true        # Broker connection events
      kafka_broker_disconnects: true     # Broker disconnection events
    
    # SASL authentication (for secured clusters)
    auth:
      enabled: false
      mechanism: "PLAIN"  # PLAIN, SCRAM-SHA-256, SCRAM-SHA-512
      username: ""
      password: ""
    
    # TLS configuration
    tls:
      enable: false
      ca_file: ""
      cert_file: ""
      key_file: ""
      insecure_skip_verify: false
    
    # Error handling with exponential backoff
    error_backoff:
      enabled: true
      initial_interval: "1s"
      max_interval: "30s"
      multiplier: 2.0
      jitter: 0.1

  jfr:
    enabled: false
    input_dirs:
      - "/var/log/jfr"
    recursive: true
    output_dir: "/var/log/jfr-json"
    poll_interval: "5s"
    sample_interval_ms: 10
    jfr_command: "jfr"
    workers: 2
    pretty_json: false
# =============================================================================
# eBPF Profiling Configuration
# =============================================================================
profiling:
  enabled: true
  
  # CPU profiling samples on-CPU execution
  cpu:
    enabled: true
    sample_rate: 99            # Hz (samples per second)
    max_stack_depth: 127       # Maximum stack frames
  
  # Off-CPU profiling captures blocking/sleep time
  off_cpu:
    enabled: true
    min_block_time_ns: 1000000  # 1ms minimum block time
  
  # Memory profiling tracks allocations
  memory:
    enabled: true
    min_alloc_size: 1024       # Track allocations >= 1KB
  
  # Mutex profiling captures lock contention
  mutex:
    enabled: true
    contention_threshold_ns: 1000000  # 1ms minimum contention
  
  # Wall clock profiling
  wall:
    enabled: false
  
  # Collection settings
  collection_interval: "10s"   # Native collection interval
  upload_interval: "60s"       # Export interval
  
  # Symbol resolution
  symbols:
    cache_size: 10000
    debug_info_enabled: true
    demangling_enabled: true
    go_symbols: true
    kernel_symbols: true
  
  # Export profiles as OTLP Logs (detailed per-sample data)
  log_export:
    enabled: false
    endpoint: "http://localhost:4318/v1/logs"
    headers: {}
    compression: "gzip"
    timeout: "30s"
    batch_size: 100
    flush_interval: "10s"
    include_stack_trace: true
  
  # Export profiles as OTLP Metrics (aggregated profiling metrics)
  # Generates metrics like:
  #   - profiler.cpu.samples (count of CPU samples)
  #   - profiler.cpu.duration_seconds (histogram)
  #   - profiler.offcpu.duration_seconds (histogram)
  #   - profiler.memory.allocations (count)
  #   - profiler.memory.allocation_bytes (total bytes)
  #   - profiler.memory.allocation_size_bytes (histogram)
  #   - profiler.mutex.contentions (count)
  #   - profiler.mutex.wait_time_seconds (histogram)
  metrics_export:
    enabled: false
    endpoint: "http://localhost:4318/v1/metrics"
    headers: {}
    compression: "gzip"
    timeout: "30s"
    # Histogram buckets for duration metrics (seconds): 1ms to 60s
    histogram_buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30, 60]
    # Histogram buckets for memory metrics (bytes): 64B to 64MB
    memory_histogram_buckets: [64, 256, 1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216, 67108864]
    # Include process attributes (pid, executable name)
    include_process_attributes: true
    # Include stack attributes (top function, class)
    include_stack_attributes: true